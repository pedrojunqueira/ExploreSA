{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple CNN to classify geophysics dataset\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path of the train dataset\n",
    "trainpath = '/Users/pedrojunqueira/Desktop/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with the id of the stamps\n",
    "trainids = os.listdir(trainpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1863"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check len - should be 1863\n",
    "len(trainids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stamp geodataframe path\n",
    "stamps_data = '../data/stamp_locations.geo.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create geodataframe objects\n",
    "stamps = gpd.read_file(stamps_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for each stamp id the label of the commodity\n",
    "id_label = dict()\n",
    "\n",
    "for i, label in zip(stamps['id'].tolist(),stamps['stratification_label'].tolist()):\n",
    "    id_label[i] = label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing stamp that does not have trainset\n",
    "id_train = dict()\n",
    "\n",
    "for item in trainids:\n",
    "    label = id_label.get(item)\n",
    "    if label == None:\n",
    "        id_train[item] = 'none'\n",
    "    else:\n",
    "        id_train[item] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1863 images processed\n",
      "500/1863 images processed\n",
      "1000/1863 images processed\n",
      "1500/1863 images processed\n",
      "1863/1863 images processed\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "\n",
    "geophysics_image = 'geophysics/gravity/isostatic_residual_gravity_anomaly.tif'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i, stampid in enumerate(trainids):\n",
    "    image_path = os.path.join(trainpath,stampid,geophysics_image)\n",
    "    label = id_train[stampid]\n",
    "    try:\n",
    "        with rasterio.open(image_path, 'r') as src:\n",
    "            image_data = src.read(1)\n",
    "            data.append(image_data)\n",
    "            labels.append(label)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        continue\n",
    "\n",
    "    if i%500 == 0:\n",
    "            print(f'{i}/{len(trainids)} images processed')\n",
    "print(f'{i+1}/{len(trainids)} images processed')\n",
    "\n",
    "dataset = (np.array(data), np.array(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack X, y from the dataset\n",
    "X, y = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cu' 'other' 'Au' 'none' 'PGE' 'Fe' 'Pb']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 0, 3, 3])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# factorizing labels to integers\n",
    "factor = pd.factorize(y)\n",
    "y = factor[0]\n",
    "definitions = factor[1]\n",
    "print(definitions)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale image features 0-1\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i,:,:] = min_max_scaler.fit_transform(X[i,:,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test\n",
    "trainX, testX, trainY, testY = train_test_split(X, y,\n",
    "                                                  test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape for cnn ingest and also chenge datatype to float\n",
    "trainX = trainX.reshape((len(trainX), 500, 500, 1))\n",
    "trainX = trainX.astype('float32')\n",
    "testX = testX.reshape((len(testX), 500, 500, 1))\n",
    "testX = testX.astype('float32')\n",
    "\n",
    "\n",
    "trainY = to_categorical(trainY)\n",
    "trainY = trainY.astype('float32')\n",
    "\n",
    "testY = to_categorical(testY)\n",
    "testY = testY.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1397, 500, 500, 1) float32\n",
      "(1397, 7) float32\n",
      "(466, 500, 500, 1) float32\n",
      "(466, 7) float32\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape,trainX.dtype)\n",
    "print(trainY.shape,trainY.dtype)\n",
    "print(testX.shape,testX.dtype)\n",
    "print(testY.shape,testX.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(500, 500, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 498, 498, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 249, 249, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 247, 247, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 123, 123, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 121, 121, 64)      36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 937024)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                59969600  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 60,025,799\n",
      "Trainable params: 60,025,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1397 samples\n",
      "Epoch 1/5\n",
      "1397/1397 [==============================] - 627s 449ms/sample - loss: 8.1044 - accuracy: 0.1625\n",
      "Epoch 2/5\n",
      "1397/1397 [==============================] - 666s 477ms/sample - loss: 1.7876 - accuracy: 0.3307\n",
      "Epoch 3/5\n",
      "1397/1397 [==============================] - 656s 470ms/sample - loss: 1.7131 - accuracy: 0.3815\n",
      "Epoch 4/5\n",
      "1397/1397 [==============================] - 655s 469ms/sample - loss: 1.5425 - accuracy: 0.4145\n",
      "Epoch 5/5\n",
      "1397/1397 [==============================] - 714s 511ms/sample - loss: 1.4613 - accuracy: 0.4896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1a6a9490>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 30s 64ms/sample - loss: 1.6582 - accuracy: 0.3927\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict_classes(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 0, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 0, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 2, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 0, 3, 3, 3, 2, 2, 3, 3,\n",
       "       3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3,\n",
       "       3, 3, 2, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 3,\n",
       "       3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pedrojunqueira/anaconda3/envs/exploration/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/simple_cnn/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/simple_cnn') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:exploration] *",
   "language": "python",
   "name": "conda-env-exploration-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
